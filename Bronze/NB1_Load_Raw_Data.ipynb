{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aac7891-075d-4e62-b786-9ebf2e2c70ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Definición de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfdd75fb-6237-4373-a035-8b2033a0da85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# catalog = \"sales\"\n",
    "# bronze_schema = \"sales_bronze\"\n",
    "# bronze_table = \"sales_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601e0214-199d-4f3b-8dbb-3e95218a56bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Obtención de los parametros\n",
    "try:\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "    bronze_schema = dbutils.widgets.get(\"bronze_schema\")\n",
    "    bronze_table = dbutils.widgets.get(\"bronze_table\")\n",
    "    path_process = \"/Volumes/sales/sales_bronze/files/process/\"\n",
    "    path_processed = \"/Volumes/sales/sales_bronze/files/processed/\"\n",
    "    print(\"Parámetros cargados exitosamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: No se pudieron obtener los parámetros\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    # Detiene la ejecución del notebook si los parámetros fallan\n",
    "    raise Exception(\"Error al obtener parámetros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba6e373-9d6c-4736-b58f-9246ab54f344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cargar Dataframe de Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e4111f-faed-4160-be94-a931e99b7116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "try:\n",
    "  files = dbutils.fs.ls(path_process)\n",
    "\n",
    "  #Solo cargo archivos csv\n",
    "  csv_files = [f.path for f in files if f.path.endswith('.csv')]\n",
    "\n",
    "  #Si no hay archivos csv, termino el notebook\n",
    "  if not csv_files:\n",
    "    raise Exception(\"Files Not Found: No se encontraron archivos .csv en el directorio\")\n",
    "\n",
    "  #Analizo solamente los archivos que tengan estructura YYYYMMDD-YYYYMMDD_Sales.csv en su nombre\n",
    "  pattern = re.compile(r\".*/(\\d{8})-(\\d{8})_Sales\\.csv$\")\n",
    "\n",
    "  matched_files = []\n",
    "  for f in csv_files:\n",
    "    if pattern.match(f):\n",
    "      matched_files.append(f)\n",
    "    else:\n",
    "      print(f\"El archivo {f} no coincide con el patrón de nombre y será ignorado.\")\n",
    "\n",
    "  #Si ningun archivo cumple con el patron, termino el notebook\n",
    "  if not matched_files:\n",
    "    raise Exception(\"Files Not Found: Ningún archivo CSV coincidió con el patrón de nombre requerido.\")\n",
    "\n",
    "  file_names = [os.path.basename(f) for f in matched_files]\n",
    "  print(f\"Archivos encontrados: {', '.join(file_names)}\")\n",
    "  df_raw = spark.read.csv(matched_files, header=True, inferSchema=False)\n",
    "  print(f\"Archivos cargados exitosamente: {', '.join(file_names)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: Fallo durante la búsqueda o lectura de archivos.\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    # Detiene el notebook\n",
    "    raise Exception(\"Error al leer los archivos de origen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4698b50d-7f67-4bf3-8ad5-2b8ad550b42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfa4cc8-27cd-4c51-bc33-ee479ee47d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark.sql.functions import expr\n",
    "\n",
    "    #Dada la existencia de registros inválidos, se aplica un filtro para asegurar que solo se carguen registros válidos\n",
    "    valid_cond = (\n",
    "        # Columnas que deben ser de tipo entero\n",
    "        expr(\"try_cast(category as int) is not null\") &\n",
    "        expr(\"try_cast(vendor_no as int) is not null\") &\n",
    "        expr(\"try_cast(itemno as int) is not null\") &\n",
    "        # Columns que deben ser de tipo string\n",
    "        expr(\"try_cast(category_name as int) is null\") &\n",
    "        expr(\"try_cast(vendor_name as int) is null\") &\n",
    "        expr(\"try_cast(im_desc as int) is null\") &\n",
    "        expr(\"try_cast(county as int) is null\")\n",
    "    )\n",
    "\n",
    "    df_final = df_raw.filter(valid_cond)\n",
    "    print(f\"Filtro completado. Registros válidos: {df_final.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: Fallo durante el filtrado de datos válidos.\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    raise Exception(\"Error en la lógica de transformación. Verifique las condiciones del filtro.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae59672b-c030-4ad3-9682-6146dea86403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Crear Raw_Sales si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9c4041-5dd5-456d-be56-23c1065c570b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "  #Determino las columnas y sus correspondientes tipos de datos dentro del df\n",
    "  columns = [f\"`{field.name}` {field.dataType.simpleString().upper()}\" for field in df_final.schema.fields]\n",
    "  columns.append(\"StoreDay TIMESTAMP\")\n",
    "\n",
    "  #Query de creacion de tabla en caso de que no exista\n",
    "  create_query = f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {catalog}.{bronze_schema}.{bronze_table} (\n",
    "    {', '.join(columns)}\n",
    "  )\n",
    "  USING DELTA\n",
    "  \"\"\"\n",
    "  spark.sql(create_query)\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: Fallo durante la creación de la tabla.\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    raise Exception(\"Verifique la sintaxis de CREATE TABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854f254e-723f-4cbf-a3be-36fb08d293b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Insertar Datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2566b5-098b-4d8e-8aaa-2cc604f26a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "  from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "  # Trunca la tabla destino antes de insertar los datos\n",
    "  spark.sql(f\"TRUNCATE TABLE {catalog}.{bronze_schema}.{bronze_table}\")\n",
    "  print(\"Tabla destino truncada.\")\n",
    "\n",
    "  # Agrega la columna StoreDay con el valor timestamp actual\n",
    "  df_insert = df_final.withColumn(\"StoreDay\", current_timestamp())\n",
    "\n",
    "  # Define los nombres de columnas de destino en el orden correcto\n",
    "  dest_cols = [\n",
    "    \"invoice_line_no\",\n",
    "    \"date\",\n",
    "    \"store\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"city\",\n",
    "    \"zipcode\",\n",
    "    \"store_location\",\n",
    "    \"county_number\",\n",
    "    \"county\",\n",
    "    \"category\",\n",
    "    \"category_name\",\n",
    "    \"vendor_no\",\n",
    "    \"vendor_name\",\n",
    "    \"itemno\",\n",
    "    \"im_desc\",\n",
    "    \"pack\",\n",
    "    \"bottle_volume_ml\",\n",
    "    \"state_bottle_cost\",\n",
    "    \"state_bottle_retail\",\n",
    "    \"sale_bottles\",\n",
    "    \"sale_dollars\",\n",
    "    \"sale_liters\",\n",
    "    \"sale_gallons\",\n",
    "    \"`:@computed_region_3r5t_5243`\",\n",
    "    \"`:@computed_region_wnea_7qqw`\",\n",
    "    \"`:@computed_region_i9mz_6gmt`\",\n",
    "    \"`:@computed_region_uhgg_e8y2`\",\n",
    "    \"`:@computed_region_e7ym_nrbf`\",\n",
    "    \"StoreDay\"\n",
    "  ]\n",
    "\n",
    "  # Crea una vista temporal\n",
    "  df_insert.select(dest_cols).createOrReplaceTempView(\"tmp_sales_insert\")\n",
    "\n",
    "  # Inserta los datos usando SQL\n",
    "  spark.sql(f\"\"\"\n",
    "  INSERT INTO {catalog}.{bronze_schema}.{bronze_table} ({', '.join(dest_cols)})\n",
    "  SELECT {', '.join(dest_cols)} FROM tmp_sales_insert\n",
    "  \"\"\")\n",
    "  print(f\"Inserción de datos completada. {df_insert.count()} filas insertadas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: Fallo durante la carga de datos\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    raise Exception(\"Error en TRUNCATE/INSERT. Verifique desfase de esquema.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6d7139-df26-420d-89e9-bc799e481372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mover archivos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4296ca7e-a73a-40a7-b6f3-62d171b59e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import os\n",
    "  from datetime import datetime\n",
    "\n",
    "  # obtiene la fecha actual en formato yyyyMMdd (ej: 20251018)\n",
    "  fecha_actual = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "  for f in matched_files:\n",
    "    # obtiene sólo el nombre del archivo (sin directorio)\n",
    "    base_name = os.path.basename(f)\n",
    "\n",
    "    # separa nombre y extensión\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "\n",
    "    # construye el nuevo nombre agregando la fecha\n",
    "    new_name = f\"{name}_{fecha_actual}{ext}\"\n",
    "\n",
    "    # arma la ruta destino\n",
    "    dest_path = os.path.join(path_processed, new_name)\n",
    "\n",
    "    # mueve el archivo\n",
    "    dbutils.fs.mv(f, dest_path)\n",
    "    print(f\"Archivo {base_name} movido a carpeta de archivos procesados\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal: Fallo durante el movimiento de archivos procesados.\")\n",
    "    print(f\"Detalle del error: {e}\")\n",
    "    raise Exception(\"Error al mover archivos (dbutils.fs.mv)\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8745507067384229,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NB1_Load_Raw_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
